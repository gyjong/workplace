{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b3c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_228/54035449.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.73      0.82      0.77       390\n",
      "          사회       0.80      0.81      0.80       439\n",
      "       생활/문화       0.76      0.66      0.71       455\n",
      "\n",
      "    accuracy                           0.76      1284\n",
      "   macro avg       0.76      0.76      0.76      1284\n",
      "weighted avg       0.76      0.76      0.76      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data_add.csv\"\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "\n",
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "# 중복된 샘플들을 제거합니다.\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "from konlpy.tag import Mecab  \n",
    "tokenizer = Mecab()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.73      0.82      0.77       390\n",
    "#           사회       0.80      0.81      0.80       439\n",
    "#        생활/문화       0.76      0.66      0.71       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.76      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.76      1284\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "# tokenizer = Komoran()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.72      0.83      0.77       390\n",
    "#           사회       0.81      0.79      0.80       439\n",
    "#        생활/문화       0.75      0.68      0.71       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.77      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.76      1284\n",
    "\n",
    "\n",
    "# from konlpy.tag import Okt             \n",
    "# tokenizer = Okt()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.72      0.83      0.77       390\n",
    "#           사회       0.79      0.79      0.79       439\n",
    "#        생활/문화       0.76      0.66      0.71       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.76      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.75      1284\n",
    "\n",
    "# from konlpy.tag import Kkma\n",
    "# tokenizer = Kkma()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.71      0.83      0.76       390\n",
    "#           사회       0.82      0.80      0.81       439\n",
    "#        생활/문화       0.75      0.66      0.70       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.76      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.76      1284\n",
    "\n",
    "\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수',\n",
    "             '무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스',\n",
    "             '날', '무상', '원', '억', '도', '어', '나', '것', '살',\n",
    "             '야', '만', '중', '라', '아', '기', '찬', '간', '상', '개', '곳', '어', '로', '합니다', '와',\n",
    "             '년', '새', '서', '화', '공개', '환', '화', '서']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data):\n",
    "    text_data = []\n",
    "\n",
    "    for index, sentence in enumerate(data):\n",
    "        try:\n",
    "            #- 토큰화\n",
    "            temp_data = tokenizer.morphs(sentence)\n",
    "            #- 불용어 제거\n",
    "            temp_data = [word for word in temp_data if word not in stopwords]\n",
    "            text_data.append(' '.join(temp_data))\n",
    "        except Exception as e:\n",
    "#             print(f\"Error at index {index}: {sentence}\\nError: {e}\")\n",
    "            text_data.append('')  # Append an empty string or some placeholder\n",
    "            continue  # Continue with the next iteration\n",
    "\n",
    "    return text_data\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c646fac",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- stopwords 추가에 따른 성능 향상은 미미 하였습니다.\n",
    "- Mecab외 Komoran, Okt, Kkma 사용하였으며, 소숫점 두자리 정도에서 성능차이가 발생 하였습니다.\n",
    "- 초기에 2023년 11월 28일 데이터에 2024년 1월 10일 데이터를 추가하였으며 F1 5% 정도 성능이 향상 되었습니다.\n",
    "- 가장 어려웠던 부분은 데이터 전처리 과정에서 오류가 발생한 부분을 확인하고 try - exception 처리하는 과정 이었습니다.\n",
    "- pandas 학습을 통하여 데이터의 처리를 좀더 자유롭게 하면 좋을것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051044e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
