{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101번 코드에 대한 데이터를 만들었습니다.\n",
      "102번 코드에 대한 데이터를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "#도구들을 임포트합니다.\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from konlpy.tag import Mecab\n",
    "from matplotlib import rc\n",
    "\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "    urllist= []\n",
    "    for i in range(1, page_num + 1):\n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "        news = requests.get(url, headers=headers)\n",
    "\n",
    "        # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "        soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "        # CASE 1\n",
    "        news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "        # CASE 2\n",
    "        news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "        # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "        for line in news_list:\n",
    "            urllist.append(line.a.get('href'))\n",
    "    return urllist\n",
    "\n",
    "\n",
    "idx2word = {'100' : '정치', '101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "    text_list = []\n",
    "    for url in urllist:\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text_list.append(article.title)\n",
    "\n",
    "    #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "    df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "    #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "    df['code'] = idx2word[str(code)]\n",
    "    return df\n",
    "\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_total_data(page_num, code_list, date):\n",
    "    df = None\n",
    "\n",
    "    for code in code_list:\n",
    "        url_list = make_urllist(page_num, code, date)\n",
    "        df_temp = make_data(url_list, code)\n",
    "        print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "        if df is not None:\n",
    "            df = pd.concat([df, df_temp])\n",
    "        else:\n",
    "            df = df_temp\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 대상 카테고리 코드\n",
    "# 100: 정치, 101: 경제, 102: 사회, 103: 생활/문화, 105: IT/과학\n",
    "code_list = [101, 102, 103, 105]\n",
    "\n",
    "\n",
    "# 특정 날짜 대상 News Category의 News Dataframe 생성\n",
    "df = make_total_data(100, code_list, 20240105)\n",
    "\n",
    "\n",
    "# 데이터프레임 파일을 csv 파일로 저장합니다.\n",
    "# 저장경로는 이번 프로젝트를 위해 만든 폴더로 지정해 주세요.\n",
    "csv_path = \"/Users/kenny_jung/aiffel/news_crawler/news_data_0105.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    print('{} File Saved!'.format(csv_path))\n",
    "\n",
    "\n",
    "#데이터 전처리\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "\n",
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "\n",
    "# csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "csv_path = \"/Users/kenny_jung/aiffel/news_crawler/news_data.csv\"\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "\n",
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "# 중복된 샘플들을 제거합니다.\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "from konlpy.tag import Mecab  \n",
    "tokenizer = Mecab('/opt/homebrew/lib/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.73      0.82      0.77       390\n",
    "#           사회       0.80      0.81      0.80       439\n",
    "#        생활/문화       0.76      0.66      0.71       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.76      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.76      1284\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "# tokenizer = Komoran()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.72      0.83      0.77       390\n",
    "#           사회       0.81      0.79      0.80       439\n",
    "#        생활/문화       0.75      0.68      0.71       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.77      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.76      1284\n",
    "\n",
    "\n",
    "# from konlpy.tag import Okt             \n",
    "# tokenizer = Okt()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.72      0.83      0.77       390\n",
    "#           사회       0.79      0.79      0.79       439\n",
    "#        생활/문화       0.76      0.66      0.71       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.76      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.75      1284\n",
    "\n",
    "# from konlpy.tag import Kkma\n",
    "# tokenizer = Kkma()\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        IT/과학       0.71      0.83      0.76       390\n",
    "#           사회       0.82      0.80      0.81       439\n",
    "#        생활/문화       0.75      0.66      0.70       455\n",
    "\n",
    "#     accuracy                           0.76      1284\n",
    "#    macro avg       0.76      0.76      0.76      1284\n",
    "# weighted avg       0.76      0.76      0.76      1284\n",
    "\n",
    "\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수', '무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스', '날', '무상', '원', '억', '도', '어', '나', '것', '살', '야', '만', '중', '라', '아', '기', '찬', '간', '상', '개', '곳', '어', '로', '합니다', '와', '년', '새', '서', '화', '공개', '환', '화', '서']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data):\n",
    "    text_data = []\n",
    "\n",
    "    for index, sentence in enumerate(data):\n",
    "        try:\n",
    "            #- 토큰화\n",
    "            temp_data = tokenizer.morphs(sentence)\n",
    "            #- 불용어 제거\n",
    "            temp_data = [word for word in temp_data if word not in stopwords]\n",
    "            text_data.append(' '.join(temp_data))\n",
    "        except Exception as e:\n",
    "#             print(f\"Error at index {index}: {sentence}\\nError: {e}\")\n",
    "            text_data.append('')  # Append an empty string or some placeholder\n",
    "            continue  # Continue with the next iteration\n",
    "\n",
    "    return text_data\n",
    "\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "# 전체 레이블 개수와 훈련용 레이블 개수를 확인합니다.\n",
    "print('훈련용 뉴스 기사의 개수   :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수      : ', len(y_train))\n",
    "print('테스트용 레이블의 개수    : ', len(y_test))\n",
    "print('전체 레이블의 개수        : ', len(y_train)+len(y_test))\n",
    "\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "df['code'].value_counts().plot(kind = 'bar')\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
